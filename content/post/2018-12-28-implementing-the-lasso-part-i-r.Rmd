---
title: 'Implementing the Lasso, Part I: R'
author: Øystein Sørensen
date: '2018-12-28'
slug: implementing-the-lasso-part-i-r
categories:
  - Machine Learning
  - Statistics
tags:
  - Lasso2019
image:
  caption: ''
  focal_point: ''
---

In a [previous post](http://osorensen.rbind.io/post/lasso-in-12-languages/), I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to `library()`. This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does. Also, anyone who has tried to convince an IT department in a large corporation to install R packages, or installing a package with lots of dependencies on a high-performance computing cluster, knows the value of base R.

The whole point of this is my wish to improve my coding skills, but feel free to read on if interested!

## Download Data

First, we need to download the [Boston housing dataset](http://lib.stat.cmu.edu/datasets/boston), which we are going to analyze. We start by reading in all the lines.

```{r}
vars <- scan(file = "http://lib.stat.cmu.edu/datasets/boston", 
             what = character(), sep = "\n", strip.white = TRUE)
```

Next, we extract the variable names, which start at the line "Variables in order". We deleted everything up to this index.

```{r}
start_index <- grep("Variables in order", vars)
vars <- vars[- seq(1, start_index, 1)]
```

Starting here, we add variable names until we reach a numeric. Performance of this tiny loop is not an issue, so I am growing the vector.

```{r}
names <- character()
while(TRUE){
  # Find location of first non-upper case letter
  end <- regexpr("[^[:upper:]]", vars[[1]])
  # Break out of loop if first letter is not uppercase
  if(end <= 1) {
    break
  } else {
    new_name <- substr(vars[[1]], 1, end - 1)
    names <- c(names, new_name)  
  }
  # Delete the item, and move to the next
  vars <- vars[-1]
}
```

These are the variable names:

```{r}
names
```

Now we have come to the numeric values, which are printed below:

```{r}
head(vars)
```

There is one issue here, namely that each data point takes up two lines. We handle this by reading the number of variables in `vars` into each row.

We first split the values by one or more space.

```{r}
values <- strsplit(vars, split = "[[:space:]]+")
head(values, 2)
```

We then unlist and convert to a numeric vector.

```{r}
values <- as.numeric(unlist(values))
head(values)
```


Finally, we convert to a matrix and set the column names.

```{r}
values <- matrix(values, ncol = length(names), byrow = TRUE,
                 dimnames = list(NULL, names))
```

Now we have our dataset:

```{r}
head(values)
```

We can finish off by extracting the response $y$ and the covariates $X$:

```{r}
X <- values[, setdiff(colnames(values), "MEDV")]
y <- values[, "MEDV"]
```


## Standardize Variables

Considering the lasso criterion,

$$\hat{\beta} = \text{arg}~\text{min}_{\beta} 1/(2n)\left\| y - X \beta\right\|_{2}^{2} + \lambda \left\|\beta\right\|_{1}$$
it is clear the if the columns of $X$ do not have equal variance, then the columns with high variance will be penalized more than the columns with low variance, since the regularization parameter $\lambda$ is the same for all of them. The common way of dealing with this is by a reparametrization First, I create a function which standardizes the variables, [using exactly the same transformations as glmnet](https://stackoverflow.com/questions/23686067/default-lambda-sequence-in-glmnet-for-cross-validation).

Also, the value of $\lambda$ yielding the zero solution is given by

$$\lambda_{max} = max_{j}\left|x_{j}^{T} y\right|/n$$
which we can see by differentiating the criterion and setting $\beta=0$. Hence, we also compute $\lambda_{max}$ in this function.

```{r}
standardize <- function(X, y){
  mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
  X <- scale(X, scale = apply(X, 2, mysd))
  y <- scale(y, scale = mysd(y))
  lambda_max <- max(abs(colSums(as.matrix(X) * as.vector(y)))) / nrow(X)
  return(list(X = X, y = y, lambda_max = lambda_max))
}
```

Here is an example of how it works:

```{r}
stvars <- standardize(X, y)
```

The column averages are kept in the attribute `scaled:center`.

```{r}
attr(stvars$X, "scaled:center")
```

The standard devitations are kept in `scaled:scale`:

```{r}
attr(stvars$X, "scaled:scale")
```

The same applies to $y$.

```{r}
attr(stvars$y, "scaled:center")
```

```{r}
attr(stvars$y, "scaled:scale")
```

An important aspect is that the intercept becomes zero when we have centered all the variables. We can illustrate this with the following linear model.

```{r}
lmfit <- with(stvars, lm(y ~ X))
coef(lmfit)
```

Next, given coefficient estimates, we need to transform them back to their original scale. The formulas are well described [here](https://stats.stackexchange.com/questions/235057/convert-standardized-coefficients-to-unstandardized-metric-coefficients-for-li), so I won't repeat them.

```{r}
transform_beta <- function(beta, X, y){
  intercept <- attr(y, "scaled:center") - 
    sum(beta * attr(X, "scaled:center") / attr(X, "scaled:scale")) *
    attr(y, "scaled:scale")
  
  c(INTERCEPT = intercept, 
    attr(y, "scaled:scale") * (beta / attr(X, "scaled:scale")))
}
```



These are the transformed coefficients. Note the we do not include the estimated intercept, which is zero.

```{r}
(beta1 <- with(stvars, transform_beta(coef(lmfit)[-1], X, y)))
```

These are the ones obtained with untransformed data.

```{r}
(beta2 <- coef(lm(y ~ X)))
```

They are equal, so the transformation seems to work.

```{r}
max(abs(beta1 - beta2))
```


## Coordinate Descent

The main part of the coordinate descent algorithm is the soft-thresholding function. It is defined as $S(t, \lambda) = \text{sign}(t)(|t| - \lambda)_{+}$, [cf. ESLII, p. 93](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). We implement it as follows.

```{r}
soft_threshold <- function(t, lambda){
  test <- abs(t) - lambda
  sign(t) * ifelse(test > 0, test, 0) 
}
```

The plot below illustrates how it works. 

```{r}
lambda <- 1
t <- seq(from = -2, to = 2, by = 0.1)
plot(t, soft_threshold(t, lambda), type = "l",
     ylab = expression(S(t, lambda)),
     main = "Soft thresholding operator")
abline(a = 0, b = 0, lty = 2)
```

This also summarizes how the lasso works. If a coefficient is too small, it is set exactly to zero!

Next, we need to implement the actual coordinate descent algorithm taking $X$, $y$, and $\lambda$ as input. It returns an estimate of $\beta$. We assume the data are standardized, and hence do not estimate the intercept in this loop.

```{r}
coordinate_descent <- function(X, y, lambda, beta_init = NULL,
                               tol = 1e-7, maxit = 1000){
  # Number of coefficients
  p <- ncol(X)
  # Initial value of beta
  if(is.null(beta_init)) {
    beta <- rep(0, ncol(X))
  } else {
    beta <- beta_init
  }
  # Control parameters to avoid an infinite loop
  eps <- 100
  control <- 0
  
  while(eps > tol && control < maxit){
    beta_old <- beta
    # Loop around the coefficients
    for(j in seq(from = 1, to = p, by = 1)){
      # Current partial residual
      pres <- (y - X[, -j, drop = FALSE] %*% beta[-j])
      # Sum to soft-threshold
      t <- mean(X[, j] * pres)
      # Update current beta
      beta[j] <- soft_threshold(t, lambda)
    }
    eps <- sum(abs(beta - beta_old))
    control <- control + 1
  }
  return(beta)
}
```


We are now ready to compute the lasso.

```{r}
lasso <- function(X, y, lambda){
  # Scale and center
  stvars <- standardize(X, y)
  
  # Run the lasso for each value of lambda, using the previous value for a warm start
  beta <- matrix(nrow = ncol(X), ncol = length(lambda))
  for(i in seq_along(lambda)){
    beta[, i] <- with(stvars, 
                      coordinate_descent(X, y, lambda = lambda[[i]], 
                                         beta_init = if(i > 1) beta[, i-1] else NULL))
  }
  beta <- with(stvars, apply(beta, 2, transform_beta, X, y))
  # Return beta
  return(beta)
}
```


We try a very simple simulated example to confirm that the implementation is correct, setting $\lambda = 0$ to get the least squares solution.

```{r}
Xtest <- matrix(rnorm(1000 * 3, mean = 1), ncol = 3)
colnames(Xtest) <- letters[1:3]
ytest <- 1 + 2 * Xtest[, 2] - 3 * Xtest[, 3] + rnorm(1000, sd = 0.02)
lasso(Xtest, ytest, lambda = 0)
```

That looks very correct! We can also check if $\lambda_{max}$ is correct.

```{r}
lambda_max <- standardize(Xtest, ytest)$lambda_max
lasso(Xtest, ytest, c(lambda_max * 0.99, lambda_max))
rm(Xtest, ytest)
```

Indeed, using $0.99 \lambda_{max}$ yields one nonzero coefficients, while using $\lambda_{max}$ yields all zero coefficients, except for the intercept which we do not penalize.

## Cross Validation

In order to determine the optimal value of $\lambda$, cross validation is a good method. We do it ten-fold, by randomly splitting the data into ten folds, fitting the lasso on nine of them, and computing the prediction error on the tenth.

```{r}
cv_lasso <- function(X, y, foldid, nlambda = 100){
  # Create the lambda vector
  lambda_max <- standardize(X, y)$lambda_max
  lambda <- exp(seq(log(lambda_max), log(1e-3 * lambda_max), 
                    length.out = nlambda))
  nfolds <- max(foldid)
  # Perform cross validation
  squared_error <- matrix(nrow = nfolds, ncol = nlambda)
  for(i in seq(1, nfolds)){
    beta <- lasso(X[foldid != i, , drop = FALSE], y[foldid != i], lambda)
    
    for(j in seq(1, nlambda)){
      squared_error[i, j] <- sum((y[foldid == i] - beta[1, j] - 
                                    X[foldid == i, , drop = FALSE] %*% beta[-1, j])^2)
    }
  }
  mse <- colSums(squared_error) / nrow(X)
  lambda_min <- lambda[which.min(mse)]
  sdse <- apply(squared_error / nrow(X) * nfolds, 2, sd)
  lambda_1se <- max(lambda[mse < min(mse + sdse / sqrt(nfolds))])
  
  # Do a final fit
  beta <- lasso(X, y, lambda)
  # Find the number of nonzero values
  nz <- apply(beta, 2, function(x) sum(x != 0))
  
  # Return the values
  list(beta = beta, nz = nz,
       mse = mse, sdse = sdse, lambda = lambda, 
       lambda_min = lambda_min,
       lambda_1se = lambda_1se)
}
```

We can now compute the cross-validated lasso, where we make sure the cross-validation folds are exactly the same as with glmnet in the previous post.

```{r}
set.seed(123)
foldid <- cut(sample(nrow(X)), breaks = 10, labels = FALSE)
system.time(fit <- cv_lasso(X, y, foldid))
```

Note that it takes a whole lot longer than `cv.glmnet`. We could probably speed up a bit by adapting an *active set strategy* in the coordinate descent algorithm, and only check every, say, 50th iteration whether any of the currently zero coefficients should be nonzero, while updating the nonzero values in every iteration. Anyhow, iterations like this is not what are was built for, and shows the advantage of using compiled code.

## Plotting

We finally plot the cross-validation error curve. [This site](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781849513067/3/ch03lvl1sec07/adding-error-bars) gave good help in figuring out how to plot the errorbars, and the source code to `plot.cv.glmnet` revealed how to plot the number of nonzero coefficients on top.

```{r}
plot(log(fit$lambda), fit$mse, 
     type = "p", col = "red", pch = 20,
     xlab = expression(log(lambda)), ylab = "Mean-Squared Error",
     ylim = c(min(fit$mse - fit$sdse), max(fit$mse + fit$sdse)))
abline(v = log(fit$lambda_min), lty = 2)
abline(v = log(fit$lambda_1se), lty = 2)
arrows(x0 = log(fit$lambda), y0 = fit$mse - fit$sdse,
       x1 = log(fit$lambda), y1 = fit$mse + fit$sdse, 
       angle = 90, code = 3, length = 0.04, lwd = 0.4)
axis(side = 3, at = log(fit$lambda), labels = paste(fit$nz), 
     tick = FALSE, line = 0)
```

## Checking the Results

Finally, we compare the results to [those obtained with glmnet](http://osorensen.rbind.io/post/lasso-in-12-languages/), to check that the implementation is correct. Note that there seems to be a scaling issue with the value of $\lambda$, probably related to some division by $n$, but this has no impact on the solution.

We find the coefficients at $\lambda_{min}$.

```{r}
fit$beta[, fit$lambda == fit$lambda_min, drop = FALSE]
```

These values are practically identical to those given by glmnet.

Next, the coefficients at $\lambda_{1se}$ are:

```{r}
fit$beta[, fit$lambda == fit$lambda_1se, drop = FALSE]
```

These are also really close to the values obtained with glmnet.

Finally, we can look at the mean squared errors.

```{r}
fit$mse[fit$lambda == fit$lambda_min]
```

```{r}
fit$mse[fit$lambda == fit$lambda_1se]
```

These are also really close the the glmnet values.

## Summary

Implementing this took longer than expected! I was expecting to spend a few hours, but all the nitty gritty with scaling and transformations took days. Luckily, these details carry over to all the other languages, so it was good to go through.

Implementing algorithms is also **not** what R is good at. Rather, R is a great interface to algorithms written in compiled languages like C/C++ and Fortran. However, it was fun to do, and the whole point of this is learning, which I definitely did.

Next up is C++, which I hope to get closer to the speed of glmnet's Fortran code. I am also giving a [talk on Rcpp](https://www.meetup.com/Oslo-useR-Group/events/256805098/) for the Oslo useR! Group in February, so need to read up on C++ language anyway :-)
