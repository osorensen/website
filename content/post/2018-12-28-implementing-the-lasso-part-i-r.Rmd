---
title: 'Implementing the Lasso, Part I: R'
author: Øystein Sørensen
date: '2018-12-28'
slug: implementing-the-lasso-part-i-r
categories:
  - Machine Learning
  - Statistics
tags:
  - Lasso2019
image:
  caption: ''
  focal_point: ''
---

In a [previous post](http://osorensen.rbind.io/post/lasso-in-12-languages/), I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to `library()`. This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does. Also, anyone who has tried to convince an IT department in a large corporation to install R packages, or installing a package with lots of dependencies on a high-performance computing cluster, knows the value of base R.

## Download Data

First, we need to download the [Boston housing dataset](http://lib.stat.cmu.edu/datasets/boston), which we are going to analyze. We start by reading in all the lines.

```{r}
vars <- scan(file = "http://lib.stat.cmu.edu/datasets/boston", 
             what = character(), sep = "\n", strip.white = TRUE)
```

Next, we extract the variable names, which start at the line "Variables in order". We deleted everything up to this index.

```{r}
start_index <- grep("Variables in order", vars)
vars <- vars[- seq(1, start_index, 1)]
```

Starting here, we add variable names until we reach a numeric. Performance of this tiny loop is not an issue, so I am growing the vector.

```{r}
names <- character()
while(TRUE){
  # Find location of first non-upper case letter
  end <- regexpr("[^[:upper:]]", vars[[1]])
  # Break out of loop if first letter is not uppercase
  if(end <= 1) {
    break
  } else {
    new_name <- substr(vars[[1]], 1, end - 1)
    names <- c(names, new_name)  
  }
  # Delete the item, and move to the next
  vars <- vars[-1]
}
```

These are the variable names:

```{r}
names
```

Now we have come to the numeric values, which are printed below:

```{r}
head(vars)
```

There is one issue here, namely that each data point takes up two lines. We handle this by reading the number of variables in `vars` into each row.

We first split the values by one or more space.

```{r}
values <- strsplit(vars, split = "[[:space:]]+")
head(values, 2)
```

We then unlist and convert to a numeric vector.

```{r}
values <- as.numeric(unlist(values))
head(values)
```


Finally, we convert to a matrix and set the column names.

```{r}
values <- matrix(values, ncol = length(names), byrow = TRUE,
                 dimnames = list(NULL, names))
```

Now we have our dataset:

```{r}
head(values)
```

We can finish off by extracting the response $y$ and the covariates $X$:

```{r}
X <- values[, setdiff(colnames(values), "MEDV")]
y <- values[, "MEDV"]
```


## Standardize Variables

Considering the lasso criterion,

$$\hat{\beta} = \text{arg}~\text{min}_{\beta} \left\| y - X \beta\right\|_{2}^{2} + \lambda \left\|\beta\right\|_{1}$$
it is clear the if the columns of $X$ do not have equal variance, then the columns with high variance will be penalized more than the columns with low variance, since the regularization parameter $\lambda$ is the same for all of them. The common way of dealing with this is by a reparametrization First, I create a function which standardizes the variables, [using exactly the same transformations as glmnet](https://stackoverflow.com/questions/23686067/default-lambda-sequence-in-glmnet-for-cross-validation).


```{r}
standardize <- function(X, y){
  mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
  X <- scale(X, scale = apply(X, 2, mysd))
  y <- scale(y, scale = mysd(y))
  return(list(X = X, y = y))
}
```

Here is an example of how it works:

```{r}
stvars <- standardize(X, y)
```

The column averages are kept in the attribute `scaled:center`.

```{r}
attr(stvars$X, "scaled:center")
```

The standard devitations are kept in `scaled:scale`:

```{r}
attr(stvars$X, "scaled:scale")
```

The same applies to $y$.

```{r}
attr(stvars$y, "scaled:center")
```

```{r}
attr(stvars$y, "scaled:scale")
```

An important aspect is that the intercept becomes zero when we have centered all the variables. We can illustrate this with the following linear model.

```{r}
lmfit <- with(stvars, lm(y ~ X))
coef(lmfit)
```

Next, given coefficient estimates, we need to transform them back to their original scale. The formulas are well described [here](https://stats.stackexchange.com/questions/235057/convert-standardized-coefficients-to-unstandardized-metric-coefficients-for-li), so I won't repeat them.

```{r}
transform_beta <- function(beta, X, y){
  intercept <- attr(y, "scaled:center") - 
    sum(beta * attr(X, "scaled:center") / attr(X, "scaled:scale")) *
    attr(y, "scaled:scale")
  
  c(INTERCEPT = intercept, 
    attr(y, "scaled:scale") * (beta / attr(X, "scaled:scale")))
}
```

These are the transformed coefficients. Note the we do not include the estimated intercept, which is zero.

```{r}
(beta1 <- with(stvars, transform_beta(coef(lmfit)[-1], X, y)))
```

These are the ones obtained with untransformed data.

```{r}
(beta2 <- coef(lm(y ~ X)))
```

They are equal, so the transformation seems to work.

```{r}
max(abs(beta1 - beta2))
```


## Coordinate Descent

The main part of the coordinate descent algorithm is the soft-thresholding function. It is defined as $S(t, \lambda) = \text{sign}(t)(|t| - \lambda)_{+}$, [cf. ESLII, p. 93](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). We implement it as follows.

```{r}
soft_threshold <- function(t, lambda){
  test <- abs(t) - lambda
  sign(t) * ifelse(test > 0, test, 0) 
}
```

The plot below illustrates how it works. 

```{r}
lambda <- 1
t <- seq(from = -2, to = 2, by = 0.1)
plot(t, soft_threshold(t, lambda), type = "l",
     ylab = expression(S(t, lambda)),
     main = "Soft thresholding operator")
abline(a = 0, b = 0, lty = 2)
```

This also summarizes how the lasso works. If a coefficient is too small, it is set exactly to zero!

Next, we need to implement the actual coordinate descent algorithm taking $X$, $y$, and $\lambda$ as input. It returns an estimate of $\beta$. We assume the data are standardized, and hence do not estimate the intercept in this loop.

```{r}
coordinate_descent <- function(X, y, lambda, beta_init = NULL,
                               tol = 1e-7, maxit = 1000){
  # Number of coefficients
  p <- ncol(X)
  # Initial value of beta
  if(is.null(beta_init)) {
    beta <- rep(0, ncol(X))
  } else {
    beta <- beta_init
  }
  # Control parameters to avoid an infinite loop
  eps <- 100
  control <- 0
  
  while(eps > tol && control < maxit){
    beta_old <- beta
    # Loop around the coefficients
    for(j in seq(from = 1, to = p, by = 1)){
      # Current partial residual
      pres <- (y - X[, -j, drop = FALSE] %*% beta[-j])
      # Sum to soft-threshold
      t <- mean(X[, j] * pres)
      # Update current beta
      beta[j] <- soft_threshold(t, lambda)
    }
    eps <- sum(abs(beta - beta_old))
    control <- control + 1
  }
  return(beta)
}
```


We are now ready to compute the lasso.

```{r}
lasso <- function(X, y, lambda){
  # Scale and center
  stvars <- standardize(X, y)
  
  # Run the lasso for each value of lambda, using the previous value for a warm start
  beta <- matrix(nrow = ncol(X), ncol = length(lambda))
  for(i in seq_along(lambda)){
    beta[, i] <- with(stvars, 
                      coordinate_descent(X, y, lambda = lambda[[i]], 
                                         beta_init = if(i > 1) beta[, i-1] else NULL))
  }
  beta <- with(stvars, apply(beta, 2, transform_beta, X, y))
  # Return beta
  return(beta)
}
```


We try a very simple simulated example to confirm that the implementation is correct, setting $\lambda = 0$ to get the least squares solution.

```{r}
Xtest <- matrix(rnorm(1000 * 3, mean = 1), ncol = 3)
colnames(Xtest) <- letters[1:3]
ytest <- 1 + 2 * Xtest[, 2] - 3 * Xtest[, 3] + rnorm(1000, sd = 0.02)
lasso(Xtest, ytest, lambda = 0)
rm(Xtest, ytest)
```

That looks very correct!

## Cross Validation

In order to determine the optimal value of $\lambda$, cross validation is a good method. We do it ten-fold, by randomly splitting the data into ten folds, fitting the lasso on nine of them, and computing the prediction error on the tenth.

```{r}
cv_lasso <- function(X, y, nlambda = 100, nfolds = 10){
  # Create nfolds random folds
  foldid <- cut(sample(nrow(X)), breaks = nfolds, labels = FALSE)
  # Create the lambda vector
  lambda <- exp(seq(log(1), log(1e-6), length.out = nlambda))
  # Perform cross validation
  squared_error <- matrix(nrow = nfolds, ncol = nlambda)
  for(i in seq(1, nfolds)){
    beta <- lasso(X[foldid != i, , drop = FALSE], y[foldid != i], lambda)
    
    for(j in seq(1, nlambda)){
      squared_error[i, j] <- sum((y[foldid == i] - beta[1, j] - 
                                    X[foldid == i, , drop = FALSE] %*% beta[-1, j])^2)
    }
  }
  mse <- colSums(squared_error) / nrow(X)
  
  # Return the values
  list(mse = mse, lambda = lambda)
}
```

We can now compute the cross-validated lasso.

```{r}
fit <- cv_lasso(X, y, nlambda = 30)
```

## Plotting

We finally plot the cross-validation error curve.

```{r}
plot(log(fit$lambda), fit$mse, 
     xlim = rev(range(log(fit$lambda))),
     type = "p", col = "red",
     xlab = expression(log(lambda)), ylab = "Mean-Squared Error")
```

