---
title: 'Implementing the Lasso, Part I: R'
author: Øystein Sørensen
date: '2018-12-28'
slug: implementing-the-lasso-part-i-r
categories:
  - Machine Learning
  - Statistics
tags:
  - Lasso2019
image:
  caption: ''
  focal_point: ''
---

In a [previous post](http://osorensen.rbind.io/post/lasso-in-12-languages/), I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to `library()`. This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does. Also, anyone who has tried to convince an IT department in a large corporation to install R packages, or installing a package with lots of dependencies on a high-performance computing cluster, knows the value of base R.

## Download Data

First, we need to download the [Boston housing dataset](http://lib.stat.cmu.edu/datasets/boston), which we are going to analyze. We start by reading in all the lines.

```{r}
vars <- scan(file = "http://lib.stat.cmu.edu/datasets/boston", 
             what = character(), sep = "\n", strip.white = TRUE)
```

Next, we extract the variable names, which start at the line "Variables in order". We deleted everything up to this index.

```{r}
start_index <- grep("Variables in order", vars)
vars <- vars[- seq(1, start_index, 1)]
```

Starting here, we add variable names until we reach a numeric. Performance of this tiny loop is not an issue, so I am growing the vector.

```{r}
names <- character()
while(TRUE){
  # Find location of first non-upper case letter
  end <- regexpr("[^[:upper:]]", vars[[1]])
  # Break out of loop if first letter is not uppercase
  if(end <= 1) {
    break
  } else {
    new_name <- substr(vars[[1]], 1, end - 1)
    names <- c(names, new_name)  
  }
  # Delete the item, and move to the next
  vars <- vars[-1]
}
```

These are the variable names:

```{r}
names
```

Now we have come to the numeric values, which are printed below:

```{r}
head(vars)
```

There is one issue here, namely that each data point takes up two lines. We handle this by reading the number of variables in `vars` into each row.

We first split the values by one or more space.

```{r}
values <- strsplit(vars, split = "[[:space:]]+")
head(values, 2)
```

We then unlist and convert to a numeric vector.

```{r}
values <- as.numeric(unlist(values))
head(values)
```


Finally, we convert to a matrix and set the column names.

```{r}
values <- matrix(values, ncol = length(names), byrow = TRUE,
                 dimnames = list(NULL, names))
```

Now we have our dataset:

```{r}
head(values)
```

We can finish off by extracting the response $y$ and the covariates $X$:

```{r}
X <- values[, setdiff(colnames(values), "MEDV")]
y <- values[, "MEDV"]
```


## Standardize Variables

Considering the lasso criterion,

$$\hat{\beta} = \text{arg}~\text{min}_{\beta} \left\| y - X \beta\right\|_{2}^{2} + \lambda \left\|\beta\right\|_{1}$$
it is clear the if the columns of $X$ do not have equal variance, then the columns with high variance will be penalized more than the columns with low variance, since the regularization parameter $\lambda$ is the same for all of them. The common way of dealing with this is by a reparametrization, where we instead compute 

$$\tilde{\beta} = \sigma_{X} \odot \hat{\beta}$$

where $\sigma_{X}$ is the vector of standard deviations of the columns of $X$, and $\odot$ is the elementwise product. We standardize $X$, compute $\tilde{\beta}$ and then find $\hat{\beta}$ by dividing by $\sigma_{X}$. In addition, we would like to center all the variables by subtracting their mean and doing the same with $y$. The intercept of the model is then just the sum of $y$. The base R `scale()` function does the job. The values we need are stored in the attributes of `Xsc`:

```{r}
attr(scale(X), "scaled:scale")
attr(scale(y, scale = FALSE), "scaled:center")
```

We will actually implement this with the lasso function below.

## Coordinate Descent

The main part of the coordinate descent algorithm is the soft-thresholding function. It is defined as $S(t, \lambda) = \text{sign}(t)(|t| - \lambda)_{+}$, [cf. ESLII, p. 93](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). We implement it as follows.

```{r}
soft_threshold <- function(t, lambda){
  test <- abs(t) - lambda
  sign(t) * ifelse(test > 0, test, 0) 
}
```

The plot below illustrates how it works. 

```{r}
lambda <- 1
t <- seq(from = -2, to = 2, by = 0.1)
plot(t, soft_threshold(t, lambda), type = "l",
     ylab = expression(S(t, lambda)),
     main = "Soft thresholding operator")
abline(a = 0, b = 0, lty = 2)
```

This also summarizes how the lasso works. If a coefficient is too small, it is set exactly to zero!

Next, we need to implement the actual coordinate descent algorithm taking $X$, $y$, and $\lambda$ as input. It returns an estimate of $\beta$.

```{r}
coordinate_descent <- function(X, y, lambda, 
                               tol = 1e-7, maxit = 1000){
  # Number of coefficients
  p <- ncol(X)
  # Initial value of beta
  beta <- rep(0, ncol(X))
  # Control parameters to avoid an infinite loop
  eps <- 100
  control <- 0
  
  while(eps > tol && control < maxit){
    beta_old <- beta
    # Loop around the coefficients
    for(j in seq(from = 1, to = p, by = 1)){
      # Current partial residual
      pres <- (y - X[, -j, drop = FALSE] %*% beta[-j])
      # Sum to soft-threshold
      t <- mean(X[, j] * pres)
      # Update current beta
      beta[j] <- soft_threshold(t, lambda)
    }
    eps <- sum(abs(beta - beta_old))
    control <- control + 1
  }
  return(beta)
}
```

We are now ready to compute the lasso.

```{r}
lasso <- function(X, y, lambda){
  # Scale and center X
  X <- scale(X)
  # Center y
  y <- scale(y, scale = FALSE)
  # Run the lasso
  beta <- coordinate_descent(X, y, lambda)
  # Transform coordinates
  beta <- beta / attr(X, "scaled:scale")
  # Add intercept
  beta <- c(attr(y, "scaled:center"), beta)
  # Add names
  names(beta) <- c("Intercept", colnames(X))  
  # Return beta
  return(beta)
}
```

In the glmnet fit from the introductory blogpost, the minimized prediction error was achieved at around $\log(\lambda) = -1$. We hence try this.

```{r}
round(lasso(X, y, lambda = exp(-1)), 3)
```

Comparing to the glmnet fit, this seems to be at the right order of magnitude. Until further, the implementation seems sound, so we go on to cross-validation, which should give very close to glmnet-fit.


