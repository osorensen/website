---
title: Lasso in Twelve Languages
author: Øystein Sørensen
date: '2018-12-28'
slug: lasso-in-12-languages
categories:
  - Statistics
  - Machine Learning
tags: []
image:
  caption: ''
  focal_point: ''
---

During the holidays, I finally took the time to read [The Pragmatic Programmer](https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X/ref=sr_1_1?ie=UTF8&qid=1546001035&sr=8-1&keywords=pragmatic+programmer). The book inspired me to learn some new programming languages, so in 2019, I am going to implement [Tibshirani's lasso](http://statweb.stanford.edu/~tibs/lasso.html) in twelve different languages, some of which I know very well, and others in which I have never even said *hello world*. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the **glmnet** package.

## Lasso

The lasso is a fairly simple and very well-known machine learning algorithm. It imposes an $\ell_{1}$-penalty on the coefficients of a regression model. In its simplest case, the criterion is

$$\hat{\beta} = \text{arg}~\text{min}_{\beta} \left\| y - X \beta\right\|_{2}^{2} + \lambda \left\|\beta\right\|_{1}$$

where $\lambda$ is a regularization parameter, $y$ is a response vector, and $X$ is a matrix of regression coefficients. Because of the absolute value in the penalty term ($\|\beta\|_{1} = \sum_{j} |\beta_{j}|$), the lasso estimate $\hat{\beta}$ typically sets some of the regression coefficients exactly to zero. This yields a sparse solution, in contrast to what an $\ell_{2}$-penalty would give. This property makes the lasso really useful in a lot of settings, including in [regularization of neural networks](https://www.deeplearningbook.org/contents/regularization.html) estimation of graphical model structures, and generalized linear models. The theory of the lasso has been described in lots of papers, much of which is summarized in [this book](https://www.springer.com/la/book/9783642201912). 

## Solution Using glmnet

In R, the **glmnet** package is the main tool for computing the lasso. It is based based on a Fortran implementation of the coordinate descent. When implementing the lasso in various languages, I will use the [Boston house-price data](http://lib.stat.cmu.edu/datasets/boston), and try to predict the median house value using the other variables in the dataset. This dataset is actually also available in R through the **MASS** package, so in order to have a reference solution, we can execute the following lines:

```{r, message=FALSE}
library(glmnet)
library(glmnetUtils)
library(MASS)
fit <- cv.glmnet(medv ~ ., data = Boston)
```

This estimates the cross-validated loss function of the lasso along a grid of $\lambda$ values. We can visualize it as follows:

```{r}
plot(fit)
```

The numbers in the upper horizontal bar show how many coefficients are nonzero. The rightmost line is the value of $\lambda$ corresponding to the "one standard error rule", and we choose this one as our final value of the regularization parameter. We get the regression coefficients as follows:

```{r}
coef(fit, s = "lambda.1se")
```

In addition, we can find the cross-validation estimate of the mean squared error:

```{r}
fit$cvm[fit$lambda == fit$lambda.1se]
```

These are the values I will try to reproduce by actually implementing the lasso in twelve different languages.

## Steps Involved in Computing the Lasso

Packages/libraries/etc. for computing the lasso exist in several languages, but in order to make it a bit more interesting, I am going to implement what **glmnet** computed for me above by actually coding it. This includes:

### Data
Download the [Boston Housing Dataset](http://lib.stat.cmu.edu/datasets/boston) and convert it to a proper object.

### Coordinate Descent
Implement the [coordinate descent algorithm](https://projecteuclid.org/euclid.aoas/1206367819) for estimating $\beta$ at a given $\lambda$. This involves iterating 

$$\tilde{\beta}_{j}(\lambda) \leftarrow S\left(\sum_{i=1}^{N} x_{ij}(y_{i} - \tilde{y}_{i}^{(j)}), \lambda \right)$$
until convergence, where $S(\cdot, \cdot)$ is the soft-thresholding operator and $\tilde{y}_{i}^{(j)}$ is the current fitted value for observation $i$ with covariate $j$ removed.

### Cross-Validation
Ten-fold cross-validation for estimating the mean-squared prediction error as a function of $\lambda$. Ideally this can be done in parallel.

### Plotting
Function for plotting the cross-validation curve.

### Standardization
Standardization of covariates before using the algorithm and rescaling to get $\hat{\beta}$ in the proper scale. This is because all columns of $X$ must have the same standard deviation as long as we use a common $\lambda$.

## Plan

My current plan is to implement the lasso in the following languages, partly based on looking at [the Stack Overflow Developer Survey](https://insights.stackoverflow.com/survey/2018/#most-popular-technologies):

* [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) (must be possible...)
* [C](https://en.wikipedia.org/wiki/C_(programming_language)) (should be really fast)
* C++ (using classes and stuff, not C-style)
* [Fortran](https://en.wikipedia.org/wiki/Fortran) (really old and really fast?)
* [Java](https://en.wikipedia.org/wiki/Java_(programming_language)) 
* JavaScript
* [Julia](https://julialang.org/) (might be the next big thing?)
* [Common Lisp](https://common-lisp.net/) (because references to Lisp keep showing up in R)
* [GNU Octave](https://www.gnu.org/software/octave/)
* [Python](https://www.python.org/) (obviously)
* [R](https://www.r-project.org/)
* [Scala](https://www.scala-lang.org/) (the main language used in Apache Spark, so it must me good to know, right?)

I will right one blogpost for each, detailing the solution. The number twelwe was chosen for reasons related to the number of full moons occuring between each time the earth circles around the sun, so the absolute deadline for finishing everything is on the 31st December 2019. 