[{"authors":null,"categories":["Machine Learning","Statistics"],"content":" In a previous post, I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to library(). This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does. Also, anyone who has tried to convince an IT department in a large corporation to install R packages, or installing a package with lots of dependencies on a high-performance computing cluster, knows the value of base R.\nDownload Data First, we need to download the Boston housing dataset, which we are going to analyze. We start by reading in all the lines.\nvars \u0026lt;- scan(file = \u0026quot;http://lib.stat.cmu.edu/datasets/boston\u0026quot;, what = character(), sep = \u0026quot;\\n\u0026quot;, strip.white = TRUE) Next, we extract the variable names, which start at the line “Variables in order”. We deleted everything up to this index.\nstart_index \u0026lt;- grep(\u0026quot;Variables in order\u0026quot;, vars) vars \u0026lt;- vars[- seq(1, start_index, 1)] Starting here, we add variable names until we reach a numeric. Performance of this tiny loop is not an issue, so I am growing the vector.\nnames \u0026lt;- character() while(TRUE){ # Find location of first non-upper case letter end \u0026lt;- regexpr(\u0026quot;[^[:upper:]]\u0026quot;, vars[[1]]) # Break out of loop if first letter is not uppercase if(end \u0026lt;= 1) { break } else { new_name \u0026lt;- substr(vars[[1]], 1, end - 1) names \u0026lt;- c(names, new_name) } # Delete the item, and move to the next vars \u0026lt;- vars[-1] } These are the variable names:\nnames ## [1] \u0026quot;CRIM\u0026quot; \u0026quot;ZN\u0026quot; \u0026quot;INDUS\u0026quot; \u0026quot;CHAS\u0026quot; \u0026quot;NOX\u0026quot; \u0026quot;RM\u0026quot; \u0026quot;AGE\u0026quot; ## [8] \u0026quot;DIS\u0026quot; \u0026quot;RAD\u0026quot; \u0026quot;TAX\u0026quot; \u0026quot;PTRATIO\u0026quot; \u0026quot;B\u0026quot; \u0026quot;LSTAT\u0026quot; \u0026quot;MEDV\u0026quot; Now we have come to the numeric values, which are printed below:\nhead(vars) ## [1] \u0026quot;0.00632 18.00 2.310 0 0.5380 6.5750 65.20 4.0900 1 296.0 15.30\u0026quot; ## [2] \u0026quot;396.90 4.98 24.00\u0026quot; ## [3] \u0026quot;0.02731 0.00 7.070 0 0.4690 6.4210 78.90 4.9671 2 242.0 17.80\u0026quot; ## [4] \u0026quot;396.90 9.14 21.60\u0026quot; ## [5] \u0026quot;0.02729 0.00 7.070 0 0.4690 7.1850 61.10 4.9671 2 242.0 17.80\u0026quot; ## [6] \u0026quot;392.83 4.03 34.70\u0026quot; There is one issue here, namely that each data point takes up two lines. We handle this by reading the number of variables in vars into each row.\nWe first split the values by one or more space.\nvalues \u0026lt;- strsplit(vars, split = \u0026quot;[[:space:]]+\u0026quot;) head(values, 2) ## [[1]] ## [1] \u0026quot;0.00632\u0026quot; \u0026quot;18.00\u0026quot; \u0026quot;2.310\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0.5380\u0026quot; \u0026quot;6.5750\u0026quot; \u0026quot;65.20\u0026quot; ## [8] \u0026quot;4.0900\u0026quot; \u0026quot;1\u0026quot; \u0026quot;296.0\u0026quot; \u0026quot;15.30\u0026quot; ## ## [[2]] ## [1] \u0026quot;396.90\u0026quot; \u0026quot;4.98\u0026quot; \u0026quot;24.00\u0026quot; We then unlist and convert to a numeric vector.\nvalues \u0026lt;- as.numeric(unlist(values)) head(values) ## [1] 0.00632 18.00000 2.31000 0.00000 0.53800 6.57500 Finally, we convert to a matrix and set the column names.\nvalues \u0026lt;- matrix(values, ncol = length(names), byrow = TRUE, dimnames = list(NULL, names)) Now we have our dataset:\nhead(values) ## CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B ## [1,] 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## [2,] 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## [3,] 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## [4,] 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## [5,] 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## [6,] 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## LSTAT MEDV ## [1,] 4.98 24.0 ## [2,] 9.14 21.6 ## [3,] 4.03 34.7 ## [4,] 2.94 33.4 ## [5,] 5.33 36.2 ## [6,] 5.21 28.7 We can finish off by extracting the response \\(y\\) and the covariates \\(X\\):\nX \u0026lt;- values[, setdiff(colnames(values), \u0026quot;MEDV\u0026quot;)] y \u0026lt;- values[, \u0026quot;MEDV\u0026quot;]  Standardize Variables Considering the lasso criterion,\n\\[\\hat{\\beta} = \\text{arg}~\\text{min}_{\\beta} \\left\\| y - X \\beta\\right\\|_{2}^{2} + \\lambda \\left\\|\\beta\\right\\|_{1}\\] it is clear the if the columns of \\(X\\) do not have equal variance, then the columns with high variance will be penalized more than the columns with low variance, since the regularization parameter \\(\\lambda\\) is the same for all of them. The common way of dealing with this is by a reparametrization, where we instead compute\n\\[\\tilde{\\beta} = \\sigma_{X} \\odot \\hat{\\beta}\\]\nwhere \\(\\sigma_{X}\\) is the vector of standard deviations of the columns of \\(X\\), and \\(\\odot\\) is the elementwise product. We standardize \\(X\\), compute \\(\\tilde{\\beta}\\) and then find \\(\\hat{\\beta}\\) by dividing by \\(\\sigma_{X}\\). In addition, we would like to center all the variables by subtracting their mean and doing the same with \\(y\\). The intercept of the model is then just the sum of \\(y\\). The base R scale() function does the job. The values we need are stored in the attributes of Xsc:\nattr(scale(X), \u0026quot;scaled:scale\u0026quot;) ## CRIM ZN INDUS CHAS NOX RM ## 8.6015451 23.3224530 6.8603529 0.2539940 0.1158777 0.7026171 ## AGE DIS RAD TAX PTRATIO B ## 28.1488614 2.1057101 8.7072594 168.5371161 2.1649455 91.2948644 ## LSTAT ## 7.1410615 attr(scale(y, scale = FALSE), \u0026quot;scaled:center\u0026quot;) ## [1] 22.53281 We will actually implement this with the lasso function below.\n Coordinate Descent The main part of the coordinate descent algorithm is the soft-thresholding function. It is defined as \\(S(t, \\lambda) = \\text{sign}(t)(|t| - \\lambda)_{+}\\), cf. ESLII, p. 93. We implement it as follows.\nsoft_threshold \u0026lt;- function(t, lambda){ test \u0026lt;- abs(t) - lambda sign(t) * ifelse(test \u0026gt; 0, test, 0) } The plot below illustrates how it works.\nlambda \u0026lt;- 1 t \u0026lt;- seq(from = -2, to = 2, by = 0.1) plot(t, soft_threshold(t, lambda), type = \u0026quot;l\u0026quot;, ylab = expression(S(t, lambda)), main = \u0026quot;Soft thresholding operator\u0026quot;) abline(a = 0, b = 0, lty = 2) This also summarizes how the lasso works. If a coefficient is too small, it is set exactly to zero!\nNext, we need to implement the actual coordinate descent algorithm taking \\(X\\), \\(y\\), and \\(\\lambda\\) as input. It returns an estimate of \\(\\beta\\).\ncoordinate_descent \u0026lt;- function(X, y, lambda, tol = 1e-7, maxit = 1000){ # Number of coefficients p \u0026lt;- ncol(X) # Initial value of beta beta \u0026lt;- rep(0, ncol(X)) # Control parameters to avoid an infinite loop eps \u0026lt;- 100 control \u0026lt;- 0 while(eps \u0026gt; tol \u0026amp;\u0026amp; control \u0026lt; maxit){ beta_old \u0026lt;- beta # Loop around the coefficients for(j in seq(from = 1, to = p, by = 1)){ # Current partial residual pres \u0026lt;- (y - X[, -j, drop = FALSE] %*% beta[-j]) # Sum to soft-threshold t \u0026lt;- mean(X[, j] * pres) # Update current beta beta[j] \u0026lt;- soft_threshold(t, lambda) } eps \u0026lt;- sum(abs(beta - beta_old)) control \u0026lt;- control + 1 } return(beta) } We are now ready to compute the lasso.\nlasso \u0026lt;- function(X, y, lambda){ # Scale and center X X \u0026lt;- scale(X) # Center y y \u0026lt;- scale(y, scale = FALSE) # Run the lasso beta \u0026lt;- coordinate_descent(X, y, lambda) # Transform coordinates beta \u0026lt;- beta / attr(X, \u0026quot;scaled:scale\u0026quot;) # Add intercept beta \u0026lt;- c(attr(y, \u0026quot;scaled:center\u0026quot;), beta) # Add names names(beta) \u0026lt;- c(\u0026quot;Intercept\u0026quot;, colnames(X)) # Return beta return(beta) } In the glmnet fit from the introductory blogpost, the minimized prediction error was achieved at around \\(\\log(\\lambda) = -1\\). We hence try this.\nround(lasso(X, y, lambda = exp(-1)), 3) ## Intercept CRIM ZN INDUS CHAS NOX RM ## 22.533 -0.023 0.000 0.000 1.929 -3.728 4.266 ## AGE DIS RAD TAX PTRATIO B LSTAT ## 0.000 -0.339 0.000 0.000 -0.791 0.007 -0.517 Comparing to the glmnet fit, this seems to be at the right order of magnitude. Until further, the implementation seems sound, so we go on to cross-validation, which should give very close to glmnet-fit.\n ","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"7968c4b605048251bb24747d70cc1733","permalink":"/post/implementing-the-lasso-part-i-r/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/post/implementing-the-lasso-part-i-r/","section":"post","summary":"In a previous post, I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to library(). This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does.","tags":["Lasso2019"],"title":"Implementing the Lasso, Part I: R","type":"post"},{"authors":null,"categories":["Statistics","Machine Learning"],"content":" During the holidays, I finally took the time to read The Pragmatic Programmer. The book inspired me to learn some new programming languages, so in 2019, I am going to implement Tibshirani’s lasso in twelve different languages, some of which I know very well, and others in which I have never even said hello world. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the glmnet package.\nLasso I chose the lasso because is a fairly simple and very well-known machine learning algorithm, cited tens of thousands of times. It imposes an \\(\\ell_{1}\\)-penalty on the coefficients of a regression model. In its simplest case, the criterion is\n\\[\\hat{\\beta} = \\text{arg}~\\text{min}_{\\beta} \\left\\| y - X \\beta\\right\\|_{2}^{2} + \\lambda \\left\\|\\beta\\right\\|_{1}\\]\nwhere \\(\\lambda\\) is a regularization parameter, \\(y\\) is a response vector, and \\(X\\) is a matrix of regression coefficients. Because of the absolute value in the penalty term (\\(\\|\\beta\\|_{1} = \\sum_{j} |\\beta_{j}|\\)), the lasso estimate \\(\\hat{\\beta}\\) typically sets some of the regression coefficients exactly to zero. This yields a sparse solution, in contrast to what an \\(\\ell_{2}\\)-penalty would give, at the same time the criterion is convex, making it much more efficient than, say, best subset selection. These properties make the lasso really useful in a lot of settings, including in regularization of neural networks estimation of graphical model structures, and generalized linear models. The theory of the lasso has been described in lots of papers, much of which is summarized in this book.\n Solution Using glmnet In R, the glmnet package is the main tool for computing the lasso. It is based on a Fortran implementation of the coordinate descent. When implementing the lasso in various languages, I will use the Boston house-price data, and try to predict the median house value using the other variables in the dataset. This dataset is actually also available in R through the MASS package, so in order to have a reference solution, we can execute the following lines:\nlibrary(glmnet) library(glmnetUtils) library(MASS) fit \u0026lt;- cv.glmnet(medv ~ ., data = Boston) This estimates the cross-validated loss function of the lasso along a grid of \\(\\lambda\\) values. We can visualize it as follows:\nplot(fit) The numbers in the upper horizontal bar show how many coefficients are nonzero. The rightmost line is the value of \\(\\lambda\\) corresponding to the “one standard error rule”, and we choose this one as our final value of the regularization parameter. We get the regression coefficients as follows:\ncoef(fit, s = \u0026quot;lambda.1se\u0026quot;) ## 14 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## 1 ## (Intercept) 16.268762344 ## crim -0.019439916 ## zn . ## indus . ## chas 1.791658202 ## nox -2.085387000 ## rm 4.259590139 ## age . ## dis -0.237030301 ## rad . ## tax . ## ptratio -0.771520796 ## black 0.006407868 ## lstat -0.517661470 In addition, we can find the cross-validation estimate of the mean squared error:\nfit$cvm[fit$lambda == fit$lambda.1se] ## [1] 27.25923 These are the values I will try to reproduce by actually implementing the lasso in twelve different languages.\n Steps Involved in Computing the Lasso Packages/libraries/etc. for computing the lasso exist in several languages, but in order to make it a bit more interesting, I am going to implement what glmnet computed for me above by actually coding it. This includes:\nData Download the Boston Housing Dataset and convert it to a proper object.\n Standardization Standardization of covariates before using the algorithm and rescaling to get \\(\\hat{\\beta}\\) in the proper scale. This is because all columns of \\(X\\) must have the same standard deviation as long as we use a common \\(\\lambda\\).\n Coordinate Descent Implement the coordinate descent algorithm for estimating \\(\\beta\\) at a given \\(\\lambda\\). This involves iterating\n\\[\\tilde{\\beta}_{j}(\\lambda) \\leftarrow S\\left(\\sum_{i=1}^{N} x_{ij}(y_{i} - \\tilde{y}_{i}^{(j)}), \\lambda \\right)\\] until convergence, where \\(S(\\cdot, \\cdot)\\) is the soft-thresholding operator and \\(\\tilde{y}_{i}^{(j)}\\) is the current fitted value for observation \\(i\\) with covariate \\(j\\) removed.\n Cross-Validation Ten-fold cross-validation for estimating the mean-squared prediction error as a function of \\(\\lambda\\). Ideally this can be done in parallel.\n Plotting Function for plotting the cross-validation curve.\n  Plan My current plan is to implement the lasso in the following languages, partly based on looking at the Stack Overflow Developer Survey:\n Bash (must be possible…) C (should be really fast) C++ (using classes and stuff, not C-style) Fortran (really old and really fast?) Java JavaScript Julia (might be the next big thing, so worth learning?) Common Lisp (because references to Lisp keep showing up in R) GNU Octave Python (obviously) R Scala (the main language used in Apache Spark, so it must me good to know, right?)  And as a bonus, Microsoft Excel, obviously.\nI will write one blogpost for each, detailing the solution. The number twelwe was chosen for reasons related to the number of full moons occuring between each time the earth circles around the sun, so the absolute deadline for finishing everything is on the 31st December 2019.\n ","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"451e93cfac38783ceab213856d9f8809","permalink":"/post/lasso-in-12-languages/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/post/lasso-in-12-languages/","section":"post","summary":"During the holidays, I finally took the time to read The Pragmatic Programmer. The book inspired me to learn some new programming languages, so in 2019, I am going to implement Tibshirani’s lasso in twelve different languages, some of which I know very well, and others in which I have never even said hello world. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the glmnet package.","tags":["Lasso2019"],"title":"Lasso in Twelve Languages","type":"post"}]