[{"authors":null,"categories":["Machine Learning","Statistics"],"content":" In a previous post, I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to library(). This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does. Also, anyone who has tried to convince an IT department in a large corporation to install R packages, or installing a package with lots of dependencies on a high-performance computing cluster, knows the value of base R.\nThe whole point of this is my wish to improve my coding skills, but feel free to read on if interested!\nDownload Data First, we need to download the Boston housing dataset, which we are going to analyze. We start by reading in all the lines.\nvars \u0026lt;- scan(file = \u0026quot;http://lib.stat.cmu.edu/datasets/boston\u0026quot;, what = character(), sep = \u0026quot;\\n\u0026quot;, strip.white = TRUE) Next, we extract the variable names, which start at the line “Variables in order”. We deleted everything up to this index.\nstart_index \u0026lt;- grep(\u0026quot;Variables in order\u0026quot;, vars) vars \u0026lt;- vars[- seq(1, start_index, 1)] Starting here, we add variable names until we reach a numeric. Performance of this tiny loop is not an issue, so I am growing the vector.\nnames \u0026lt;- character() while(TRUE){ # Find location of first non-upper case letter end \u0026lt;- regexpr(\u0026quot;[^[:upper:]]\u0026quot;, vars[[1]]) # Break out of loop if first letter is not uppercase if(end \u0026lt;= 1) { break } else { new_name \u0026lt;- substr(vars[[1]], 1, end - 1) names \u0026lt;- c(names, new_name) } # Delete the item, and move to the next vars \u0026lt;- vars[-1] } These are the variable names:\nnames ## [1] \u0026quot;CRIM\u0026quot; \u0026quot;ZN\u0026quot; \u0026quot;INDUS\u0026quot; \u0026quot;CHAS\u0026quot; \u0026quot;NOX\u0026quot; \u0026quot;RM\u0026quot; \u0026quot;AGE\u0026quot; ## [8] \u0026quot;DIS\u0026quot; \u0026quot;RAD\u0026quot; \u0026quot;TAX\u0026quot; \u0026quot;PTRATIO\u0026quot; \u0026quot;B\u0026quot; \u0026quot;LSTAT\u0026quot; \u0026quot;MEDV\u0026quot; Now we have come to the numeric values, which are printed below:\nhead(vars) ## [1] \u0026quot;0.00632 18.00 2.310 0 0.5380 6.5750 65.20 4.0900 1 296.0 15.30\u0026quot; ## [2] \u0026quot;396.90 4.98 24.00\u0026quot; ## [3] \u0026quot;0.02731 0.00 7.070 0 0.4690 6.4210 78.90 4.9671 2 242.0 17.80\u0026quot; ## [4] \u0026quot;396.90 9.14 21.60\u0026quot; ## [5] \u0026quot;0.02729 0.00 7.070 0 0.4690 7.1850 61.10 4.9671 2 242.0 17.80\u0026quot; ## [6] \u0026quot;392.83 4.03 34.70\u0026quot; There is one issue here, namely that each data point takes up two lines. We handle this by reading the number of variables in vars into each row.\nWe first split the values by one or more space.\nvalues \u0026lt;- strsplit(vars, split = \u0026quot;[[:space:]]+\u0026quot;) head(values, 2) ## [[1]] ## [1] \u0026quot;0.00632\u0026quot; \u0026quot;18.00\u0026quot; \u0026quot;2.310\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0.5380\u0026quot; \u0026quot;6.5750\u0026quot; \u0026quot;65.20\u0026quot; ## [8] \u0026quot;4.0900\u0026quot; \u0026quot;1\u0026quot; \u0026quot;296.0\u0026quot; \u0026quot;15.30\u0026quot; ## ## [[2]] ## [1] \u0026quot;396.90\u0026quot; \u0026quot;4.98\u0026quot; \u0026quot;24.00\u0026quot; We then unlist and convert to a numeric vector.\nvalues \u0026lt;- as.numeric(unlist(values)) head(values) ## [1] 0.00632 18.00000 2.31000 0.00000 0.53800 6.57500 Finally, we convert to a matrix and set the column names.\nvalues \u0026lt;- matrix(values, ncol = length(names), byrow = TRUE, dimnames = list(NULL, names)) Now we have our dataset:\nhead(values) ## CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B ## [1,] 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## [2,] 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## [3,] 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## [4,] 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## [5,] 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## [6,] 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## LSTAT MEDV ## [1,] 4.98 24.0 ## [2,] 9.14 21.6 ## [3,] 4.03 34.7 ## [4,] 2.94 33.4 ## [5,] 5.33 36.2 ## [6,] 5.21 28.7 We can finish off by extracting the response \\(y\\) and the covariates \\(X\\):\nX \u0026lt;- values[, setdiff(colnames(values), \u0026quot;MEDV\u0026quot;)] y \u0026lt;- values[, \u0026quot;MEDV\u0026quot;]  Standardize Variables Considering the lasso criterion,\n\\[\\hat{\\beta} = \\text{arg}~\\text{min}_{\\beta} 1/(2n)\\left\\| y - X \\beta\\right\\|_{2}^{2} + \\lambda \\left\\|\\beta\\right\\|_{1}\\] it is clear the if the columns of \\(X\\) do not have equal variance, then the columns with high variance will be penalized more than the columns with low variance, since the regularization parameter \\(\\lambda\\) is the same for all of them. The common way of dealing with this is by a reparametrization First, I create a function which standardizes the variables, using exactly the same transformations as glmnet.\nAlso, the value of \\(\\lambda\\) yielding the zero solution is given by\n\\[\\lambda_{max} = max_{j}\\left|x_{j}^{T} y\\right|/n\\] which we can see by differentiating the criterion and setting \\(\\beta=0\\). Hence, we also compute \\(\\lambda_{max}\\) in this function.\nstandardize \u0026lt;- function(X, y){ mysd \u0026lt;- function(y) sqrt(sum((y-mean(y))^2)/length(y)) X \u0026lt;- scale(X, scale = apply(X, 2, mysd)) y \u0026lt;- scale(y, scale = mysd(y)) lambda_max \u0026lt;- max(abs(colSums(as.matrix(X) * as.vector(y)))) / nrow(X) return(list(X = X, y = y, lambda_max = lambda_max)) } Here is an example of how it works:\nstvars \u0026lt;- standardize(X, y) The column averages are kept in the attribute scaled:center.\nattr(stvars$X, \u0026quot;scaled:center\u0026quot;) ## CRIM ZN INDUS CHAS NOX ## 3.61352356 11.36363636 11.13677866 0.06916996 0.55469506 ## RM AGE DIS RAD TAX ## 6.28463439 68.57490119 3.79504269 9.54940711 408.23715415 ## PTRATIO B LSTAT ## 18.45553360 356.67403162 12.65306324 The standard devitations are kept in scaled:scale:\nattr(stvars$X, \u0026quot;scaled:scale\u0026quot;) ## CRIM ZN INDUS CHAS NOX RM ## 8.5930414 23.2993957 6.8535706 0.2537429 0.1157631 0.7019225 ## AGE DIS RAD TAX PTRATIO B ## 28.1210326 2.1036284 8.6986511 168.3704950 2.1628052 91.2046075 ## LSTAT ## 7.1340016 The same applies to \\(y\\).\nattr(stvars$y, \u0026quot;scaled:center\u0026quot;) ## [1] 22.53281 attr(stvars$y, \u0026quot;scaled:scale\u0026quot;) ## [1] 9.188012 An important aspect is that the intercept becomes zero when we have centered all the variables. We can illustrate this with the following linear model.\nlmfit \u0026lt;- with(stvars, lm(y ~ X)) coef(lmfit) ## (Intercept) XCRIM XZN XINDUS XCHAS ## 1.060774e-15 -1.010171e-01 1.177152e-01 1.533520e-02 7.419883e-02 ## XNOX XRM XAGE XDIS XRAD ## -2.238480e-01 2.910565e-01 2.118638e-03 -3.378363e-01 2.897491e-01 ## XTAX XPTRATIO XB XLSTAT ## -2.260317e-01 -2.242712e-01 9.243223e-02 -4.074469e-01 Next, given coefficient estimates, we need to transform them back to their original scale. The formulas are well described here, so I won’t repeat them.\ntransform_beta \u0026lt;- function(beta, X, y){ intercept \u0026lt;- attr(y, \u0026quot;scaled:center\u0026quot;) - sum(beta * attr(X, \u0026quot;scaled:center\u0026quot;) / attr(X, \u0026quot;scaled:scale\u0026quot;)) * attr(y, \u0026quot;scaled:scale\u0026quot;) c(INTERCEPT = intercept, attr(y, \u0026quot;scaled:scale\u0026quot;) * (beta / attr(X, \u0026quot;scaled:scale\u0026quot;))) } These are the transformed coefficients. Note the we do not include the estimated intercept, which is zero.\n(beta1 \u0026lt;- with(stvars, transform_beta(coef(lmfit)[-1], X, y))) ## INTERCEPT XCRIM XZN XINDUS XCHAS ## 3.645949e+01 -1.080114e-01 4.642046e-02 2.055863e-02 2.686734e+00 ## XNOX XRM XAGE XDIS XRAD ## -1.776661e+01 3.809865e+00 6.922246e-04 -1.475567e+00 3.060495e-01 ## XTAX XPTRATIO XB XLSTAT ## -1.233459e-02 -9.527472e-01 9.311683e-03 -5.247584e-01 These are the ones obtained with untransformed data.\n(beta2 \u0026lt;- coef(lm(y ~ X))) ## (Intercept) XCRIM XZN XINDUS XCHAS ## 3.645949e+01 -1.080114e-01 4.642046e-02 2.055863e-02 2.686734e+00 ## XNOX XRM XAGE XDIS XRAD ## -1.776661e+01 3.809865e+00 6.922246e-04 -1.475567e+00 3.060495e-01 ## XTAX XPTRATIO XB XLSTAT ## -1.233459e-02 -9.527472e-01 9.311683e-03 -5.247584e-01 They are equal, so the transformation seems to work.\nmax(abs(beta1 - beta2)) ## [1] 2.842171e-13  Coordinate Descent The main part of the coordinate descent algorithm is the soft-thresholding function. It is defined as \\(S(t, \\lambda) = \\text{sign}(t)(|t| - \\lambda)_{+}\\), cf. ESLII, p. 93. We implement it as follows.\nsoft_threshold \u0026lt;- function(t, lambda){ test \u0026lt;- abs(t) - lambda sign(t) * ifelse(test \u0026gt; 0, test, 0) } The plot below illustrates how it works.\nlambda \u0026lt;- 1 t \u0026lt;- seq(from = -2, to = 2, by = 0.1) plot(t, soft_threshold(t, lambda), type = \u0026quot;l\u0026quot;, ylab = expression(S(t, lambda)), main = \u0026quot;Soft thresholding operator\u0026quot;) abline(a = 0, b = 0, lty = 2) This also summarizes how the lasso works. If a coefficient is too small, it is set exactly to zero!\nNext, we need to implement the actual coordinate descent algorithm taking \\(X\\), \\(y\\), and \\(\\lambda\\) as input. It returns an estimate of \\(\\beta\\). We assume the data are standardized, and hence do not estimate the intercept in this loop.\ncoordinate_descent \u0026lt;- function(X, y, lambda, beta_init = NULL, tol = 1e-7, maxit = 1000){ # Number of coefficients p \u0026lt;- ncol(X) # Initial value of beta if(is.null(beta_init)) { beta \u0026lt;- rep(0, ncol(X)) } else { beta \u0026lt;- beta_init } # Control parameters to avoid an infinite loop eps \u0026lt;- 100 control \u0026lt;- 0 while(eps \u0026gt; tol \u0026amp;\u0026amp; control \u0026lt; maxit){ beta_old \u0026lt;- beta # Loop around the coefficients for(j in seq(from = 1, to = p, by = 1)){ # Current partial residual pres \u0026lt;- (y - X[, -j, drop = FALSE] %*% beta[-j]) # Sum to soft-threshold t \u0026lt;- mean(X[, j] * pres) # Update current beta beta[j] \u0026lt;- soft_threshold(t, lambda) } eps \u0026lt;- sum(abs(beta - beta_old)) control \u0026lt;- control + 1 } return(beta) } We are now ready to compute the lasso.\nlasso \u0026lt;- function(X, y, lambda){ # Scale and center stvars \u0026lt;- standardize(X, y) # Run the lasso for each value of lambda, using the previous value for a warm start beta \u0026lt;- matrix(nrow = ncol(X), ncol = length(lambda)) for(i in seq_along(lambda)){ beta[, i] \u0026lt;- with(stvars, coordinate_descent(X, y, lambda = lambda[[i]], beta_init = if(i \u0026gt; 1) beta[, i-1] else NULL)) } beta \u0026lt;- with(stvars, apply(beta, 2, transform_beta, X, y)) # Return beta return(beta) } We try a very simple simulated example to confirm that the implementation is correct, setting \\(\\lambda = 0\\) to get the least squares solution.\nXtest \u0026lt;- matrix(rnorm(1000 * 3, mean = 1), ncol = 3) colnames(Xtest) \u0026lt;- letters[1:3] ytest \u0026lt;- 1 + 2 * Xtest[, 2] - 3 * Xtest[, 3] + rnorm(1000, sd = 0.02) lasso(Xtest, ytest, lambda = 0) ## [,1] ## INTERCEPT 1.0008283517 ## a -0.0005470972 ## b 1.9997526335 ## c -2.9998055359 That looks very correct! We can also check if \\(\\lambda_{max}\\) is correct.\nlambda_max \u0026lt;- standardize(Xtest, ytest)$lambda_max lasso(Xtest, ytest, c(lambda_max * 0.99, lambda_max)) ## [,1] [,2] ## INTERCEPT 0.04207374 0.01237216 ## a 0.00000000 0.00000000 ## b 0.00000000 0.00000000 ## c -0.03010121 0.00000000 rm(Xtest, ytest) Indeed, using \\(0.99 \\lambda_{max}\\) yields one nonzero coefficients, while using \\(\\lambda_{max}\\) yields all zero coefficients, except for the intercept which we do not penalize.\n Cross Validation In order to determine the optimal value of \\(\\lambda\\), cross validation is a good method. We do it ten-fold, by randomly splitting the data into ten folds, fitting the lasso on nine of them, and computing the prediction error on the tenth.\ncv_lasso \u0026lt;- function(X, y, foldid, nlambda = 100){ # Create the lambda vector lambda_max \u0026lt;- standardize(X, y)$lambda_max lambda \u0026lt;- exp(seq(log(lambda_max), log(1e-3 * lambda_max), length.out = nlambda)) nfolds \u0026lt;- max(foldid) # Perform cross validation squared_error \u0026lt;- matrix(nrow = nfolds, ncol = nlambda) for(i in seq(1, nfolds)){ beta \u0026lt;- lasso(X[foldid != i, , drop = FALSE], y[foldid != i], lambda) for(j in seq(1, nlambda)){ squared_error[i, j] \u0026lt;- sum((y[foldid == i] - beta[1, j] - X[foldid == i, , drop = FALSE] %*% beta[-1, j])^2) } } mse \u0026lt;- colSums(squared_error) / nrow(X) lambda_min \u0026lt;- lambda[which.min(mse)] sdse \u0026lt;- apply(squared_error / nrow(X) * nfolds, 2, sd) lambda_1se \u0026lt;- max(lambda[mse \u0026lt; min(mse + sdse / sqrt(nfolds))]) # Do a final fit beta \u0026lt;- lasso(X, y, lambda) # Find the number of nonzero values nz \u0026lt;- apply(beta, 2, function(x) sum(x != 0)) # Return the values list(beta = beta, nz = nz, mse = mse, sdse = sdse, lambda = lambda, lambda_min = lambda_min, lambda_1se = lambda_1se) } We can now compute the cross-validated lasso, where we make sure the cross-validation folds are exactly the same as with glmnet in the previous post.\nset.seed(123) foldid \u0026lt;- cut(sample(nrow(X)), breaks = 10, labels = FALSE) system.time(fit \u0026lt;- cv_lasso(X, y, foldid)) ## user system elapsed ## 16.432 2.980 19.419 Note that it takes a whole lot longer than cv.glmnet. We could probably speed up a bit by adapting an active set strategy in the coordinate descent algorithm, and only check every, say, 50th iteration whether any of the currently zero coefficients should be nonzero, while updating the nonzero values in every iteration. Anyhow, iterations like this is not what are was built for, and shows the advantage of using compiled code.\n Plotting We finally plot the cross-validation error curve. This site gave good help in figuring out how to plot the errorbars, and the source code to plot.cv.glmnet revealed how to plot the number of nonzero coefficients on top.\nplot(log(fit$lambda), fit$mse, type = \u0026quot;p\u0026quot;, col = \u0026quot;red\u0026quot;, pch = 20, xlab = expression(log(lambda)), ylab = \u0026quot;Mean-Squared Error\u0026quot;, ylim = c(min(fit$mse - fit$sdse), max(fit$mse + fit$sdse))) abline(v = log(fit$lambda_min), lty = 2) abline(v = log(fit$lambda_1se), lty = 2) arrows(x0 = log(fit$lambda), y0 = fit$mse - fit$sdse, x1 = log(fit$lambda), y1 = fit$mse + fit$sdse, angle = 90, code = 3, length = 0.04, lwd = 0.4) axis(side = 3, at = log(fit$lambda), labels = paste(fit$nz), tick = FALSE, line = 0)  Checking the Results Finally, we compare the results to those obtained with glmnet, to check that the implementation is correct. Note that there seems to be a scaling issue with the value of \\(\\lambda\\), probably related to some division by \\(n\\), but this has no impact on the solution.\nWe find the coefficients at \\(\\lambda_{min}\\).\nfit$beta[, fit$lambda == fit$lambda_min, drop = FALSE] ## [,1] ## INTERCEPT 34.513305737 ## CRIM -0.098896046 ## ZN 0.041622066 ## INDUS 0.000000000 ## CHAS 2.683895646 ## NOX -16.343459403 ## RM 3.863042585 ## AGE 0.000000000 ## DIS -1.399357895 ## RAD 0.255304812 ## TAX -0.009932879 ## PTRATIO -0.930777502 ## B 0.009035310 ## LSTAT -0.522483326 These values are practically identical to those given by glmnet.\nNext, the coefficients at \\(\\lambda_{1se}\\) are:\nfit$beta[, fit$lambda == fit$lambda_1se, drop = FALSE] ## [,1] ## INTERCEPT 20.870385234 ## CRIM -0.031078538 ## ZN 0.006879888 ## INDUS 0.000000000 ## CHAS 2.206875612 ## NOX -6.916062381 ## RM 4.254274727 ## AGE 0.000000000 ## DIS -0.594231978 ## RAD 0.000000000 ## TAX 0.000000000 ## PTRATIO -0.814146380 ## B 0.007013715 ## LSTAT -0.519821160 These are also really close to the values obtained with glmnet.\nFinally, we can look at the mean squared errors.\nfit$mse[fit$lambda == fit$lambda_min] ## [1] 23.6809 fit$mse[fit$lambda == fit$lambda_1se] ## [1] 25.80286 These are also really close the the glmnet values.\n Summary Implementing this took longer than expected! I was expecting to spend a few hours, but all the nitty gritty with scaling and transformations took days. Luckily, these details carry over to all the other languages, so it was good to go through.\nImplementing algorithms is also not what R is good at. Rather, R is a great interface to algorithms written in compiled languages like C/C++ and Fortran. However, it was fun to do, and the whole point of this is learning, which I definitely did.\nNext up is C++, which I hope to get closer to the speed of glmnet’s Fortran code. I am also giving a talk on Rcpp for the Oslo useR! Group in February, so need to read up on C++ language anyway :-)\n ","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"7968c4b605048251bb24747d70cc1733","permalink":"http://osorensen.rbind.io/post/implementing-the-lasso-part-i-r/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/post/implementing-the-lasso-part-i-r/","section":"post","summary":"In a previous post, I briefly described the Lasso, and my plan for implementing it in twelve languages in 2019. To start out easily, and have a reference, I am beginning with R. I am not going to use any external dependencies, i.e., no calls to library(). This is obviously not how we we do it in real life, but really useful in order to understand what the algorithm really does.","tags":["Lasso2019"],"title":"Implementing the Lasso, Part I: R","type":"post"},{"authors":null,"categories":["Statistics","Machine Learning"],"content":" During the holidays, I finally took the time to read The Pragmatic Programmer. The book inspired me to learn some new programming languages, so in 2019, I am going to implement Tibshirani’s lasso in twelve different languages, some of which I know very well, and others in which I have never even said hello world. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the glmnet package.\nLasso I chose the lasso because is a fairly simple and very well-known machine learning algorithm, cited tens of thousands of times. It imposes an \\(\\ell_{1}\\)-penalty on the coefficients of a regression model. In its simplest case, the criterion is\n\\[\\hat{\\beta} = \\text{arg}~\\text{min}_{\\beta} \\left\\| y - X \\beta\\right\\|_{2}^{2} + \\lambda \\left\\|\\beta\\right\\|_{1}\\]\nwhere \\(\\lambda\\) is a regularization parameter, \\(y\\) is a response vector, and \\(X\\) is a matrix of regression coefficients. Because of the absolute value in the penalty term (\\(\\|\\beta\\|_{1} = \\sum_{j} |\\beta_{j}|\\)), the lasso estimate \\(\\hat{\\beta}\\) typically sets some of the regression coefficients exactly to zero. This yields a sparse solution, in contrast to what an \\(\\ell_{2}\\)-penalty would give, at the same time the criterion is convex, making it much more efficient than, say, best subset selection. These properties make the lasso really useful in a lot of settings, including in regularization of neural networks estimation of graphical model structures, and generalized linear models. The theory of the lasso has been described in lots of papers, much of which is summarized in this book.\n Solution Using glmnet In R, the glmnet package is the main tool for computing the lasso. It is based on a Fortran implementation of the coordinate descent. When implementing the lasso in various languages, I will use the Boston house-price data, and try to predict the median house value using the other variables in the dataset. This dataset is actually also available in R through the MASS package, so in order to have a reference solution, we can execute the following lines, where we create the ten-fold cross-validation outside in order to ensure that it will be the same when reimplementing it.\nlibrary(glmnet) library(glmnetUtils) library(MASS) set.seed(123) foldid \u0026lt;- cut(sample(nrow(Boston)), breaks = 10, labels = FALSE) system.time(fit \u0026lt;- cv.glmnet(medv ~ ., data = Boston, foldid = foldid)) ## user system elapsed ## 0.092 0.005 0.097 This estimates the cross-validated loss function of the lasso along a grid of \\(\\lambda\\) values. We can visualize it as follows:\nplot(fit) The numbers in the upper horizontal bar show how many coefficients are nonzero. The rightmost line is the value of \\(\\lambda\\) corresponding to the “one standard error rule”. We get the regression coefficients as follows, first at \\(\\lambda_{min}\\).\ncoef(fit, s = \u0026quot;lambda.min\u0026quot;) ## 14 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## 1 ## (Intercept) 34.594713527 ## crim -0.099226869 ## zn 0.041830020 ## indus . ## chas 2.688250324 ## nox -16.401122000 ## rm 3.861229965 ## age . ## dis -1.404571749 ## rad 0.256788019 ## tax -0.009997514 ## ptratio -0.931437290 ## black 0.009049252 ## lstat -0.522505968 Next at \\(\\lambda_{1se}\\).\ncoef(fit, s = \u0026quot;lambda.1se\u0026quot;) ## 14 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## 1 ## (Intercept) 20.443234513 ## crim -0.029579467 ## zn 0.005117086 ## indus . ## chas 2.168428935 ## nox -6.478368167 ## rm 4.261458164 ## age . ## dis -0.553162685 ## rad . ## tax . ## ptratio -0.813351925 ## black 0.006958142 ## lstat -0.519363373 In addition, we can find the cross-validation estimate of the mean squared error:\nfit$cvm[fit$lambda == fit$lambda.min] ## [1] 23.68254 fit$cvm[fit$lambda == fit$lambda.1se] ## [1] 25.91843 These are the values I will try to reproduce by actually implementing the lasso in twelve different languages.\n Steps Involved in Computing the Lasso Packages/libraries/etc. for computing the lasso exist in several languages, but in order to make it a bit more interesting, I am going to implement what glmnet computed for me above by actually coding it. This includes:\nData Download the Boston Housing Dataset and convert it to a proper object.\n Standardization Standardization of covariates before using the algorithm and rescaling to get \\(\\hat{\\beta}\\) in the proper scale. This is because all columns of \\(X\\) must have the same standard deviation as long as we use a common \\(\\lambda\\).\n Coordinate Descent Implement the coordinate descent algorithm for estimating \\(\\beta\\) at a given \\(\\lambda\\). This involves iterating\n\\[\\tilde{\\beta}_{j}(\\lambda) \\leftarrow S\\left(\\sum_{i=1}^{N} x_{ij}(y_{i} - \\tilde{y}_{i}^{(j)}), \\lambda \\right)\\] until convergence, where \\(S(\\cdot, \\cdot)\\) is the soft-thresholding operator and \\(\\tilde{y}_{i}^{(j)}\\) is the current fitted value for observation \\(i\\) with covariate \\(j\\) removed.\n Cross-Validation Ten-fold cross-validation for estimating the mean-squared prediction error as a function of \\(\\lambda\\). Ideally this can be done in parallel.\n Plotting Function for plotting the cross-validation curve.\n  Plan My current plan is to implement the lasso in the following languages, partly based on looking at the Stack Overflow Developer Survey:\n Bash (must be possible…) C (should be really fast) C++ (using classes and stuff, not C-style) Fortran (really old and really fast?) Java JavaScript Julia (might be the next big thing, so worth learning?) Common Lisp (because references to Lisp keep showing up in R) GNU Octave Python (obviously) R Scala (the main language used in Apache Spark, so it must me good to know, right?)  And as a bonus, Microsoft Excel, obviously.\nI will write one blogpost for each, detailing the solution. The number twelwe was chosen for reasons related to the number of full moons occuring between each time the earth circles around the sun, so the absolute deadline for finishing everything is on the 31st December 2019.\n ","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"451e93cfac38783ceab213856d9f8809","permalink":"http://osorensen.rbind.io/post/lasso-in-12-languages/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/post/lasso-in-12-languages/","section":"post","summary":"During the holidays, I finally took the time to read The Pragmatic Programmer. The book inspired me to learn some new programming languages, so in 2019, I am going to implement Tibshirani’s lasso in twelve different languages, some of which I know very well, and others in which I have never even said hello world. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the glmnet package.","tags":["Lasso2019"],"title":"Lasso in Twelve Languages","type":"post"}]