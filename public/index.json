[{"authors":null,"categories":["Statistics","Machine Learning"],"content":" During the holidays, I finally took the time to read The Pragmatic Programmer. The book inspired me to learn some new programming languages, so in 2019, I am going to implement Tibshirani’s lasso in twelve different languages, some of which I know very well, and others in which I have never even said hello world. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the glmnet package.\nLasso I chose the lasso because is a fairly simple and very well-known machine learning algorithm, cited tens of thousands of times. It imposes an \\(\\ell_{1}\\)-penalty on the coefficients of a regression model. In its simplest case, the criterion is\n\\[\\hat{\\beta} = \\text{arg}~\\text{min}_{\\beta} \\left\\| y - X \\beta\\right\\|_{2}^{2} + \\lambda \\left\\|\\beta\\right\\|_{1}\\]\nwhere \\(\\lambda\\) is a regularization parameter, \\(y\\) is a response vector, and \\(X\\) is a matrix of regression coefficients. Because of the absolute value in the penalty term (\\(\\|\\beta\\|_{1} = \\sum_{j} |\\beta_{j}|\\)), the lasso estimate \\(\\hat{\\beta}\\) typically sets some of the regression coefficients exactly to zero. This yields a sparse solution, in contrast to what an \\(\\ell_{2}\\)-penalty would give. This property makes the lasso really useful in a lot of settings, including in regularization of neural networks estimation of graphical model structures, and generalized linear models. The theory of the lasso has been described in lots of papers, much of which is summarized in this book.\n Solution Using glmnet In R, the glmnet package is the main tool for computing the lasso. It is based based on a Fortran implementation of the coordinate descent. When implementing the lasso in various languages, I will use the Boston house-price data, and try to predict the median house value using the other variables in the dataset. This dataset is actually also available in R through the MASS package, so in order to have a reference solution, we can execute the following lines:\nlibrary(glmnet) library(glmnetUtils) library(MASS) fit \u0026lt;- cv.glmnet(medv ~ ., data = Boston) This estimates the cross-validated loss function of the lasso along a grid of \\(\\lambda\\) values. We can visualize it as follows:\nplot(fit) The numbers in the upper horizontal bar show how many coefficients are nonzero. The rightmost line is the value of \\(\\lambda\\) corresponding to the “one standard error rule”, and we choose this one as our final value of the regularization parameter. We get the regression coefficients as follows:\ncoef(fit, s = \u0026quot;lambda.1se\u0026quot;) ## 14 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## 1 ## (Intercept) 17.471486255 ## crim -0.021826808 ## zn . ## indus . ## chas 1.897718334 ## nox -3.341293682 ## rm 4.266279454 ## age . ## dis -0.317122207 ## rad . ## tax . ## ptratio -0.787265491 ## black 0.006558204 ## lstat -0.518129464 In addition, we can find the cross-validation estimate of the mean squared error:\nfit$cvm[fit$lambda == fit$lambda.1se] ## [1] 26.96739 These are the values I will try to reproduce by actually implementing the lasso in twelve different languages.\n Steps Involved in Computing the Lasso Packages/libraries/etc. for computing the lasso exist in several languages, but in order to make it a bit more interesting, I am going to implement what glmnet computed for me above by actually coding it. This includes:\nData Download the Boston Housing Dataset and convert it to a proper object.\n Coordinate Descent Implement the coordinate descent algorithm for estimating \\(\\beta\\) at a given \\(\\lambda\\). This involves iterating\n\\[\\tilde{\\beta}_{j}(\\lambda) \\leftarrow S\\left(\\sum_{i=1}^{N} x_{ij}(y_{i} - \\tilde{y}_{i}^{(j)}), \\lambda \\right)\\] until convergence, where \\(S(\\cdot, \\cdot)\\) is the soft-thresholding operator and \\(\\tilde{y}_{i}^{(j)}\\) is the current fitted value for observation \\(i\\) with covariate \\(j\\) removed.\n Cross-Validation Ten-fold cross-validation for estimating the mean-squared prediction error as a function of \\(\\lambda\\). Ideally this can be done in parallel.\n Plotting Function for plotting the cross-validation curve.\n Standardization Standardization of covariates before using the algorithm and rescaling to get \\(\\hat{\\beta}\\) in the proper scale. This is because all columns of \\(X\\) must have the same standard deviation as long as we use a common \\(\\lambda\\).\n  Plan My current plan is to implement the lasso in the following languages, partly based on looking at the Stack Overflow Developer Survey:\n Bash (must be possible…) C (should be really fast) C++ (using classes and stuff, not C-style) Fortran (really old and really fast?) Java JavaScript Julia (might be the next big thing?) Common Lisp (because references to Lisp keep showing up in R) GNU Octave Python (obviously) R Scala (the main language used in Apache Spark, so it must me good to know, right?)  I will write one blogpost for each, detailing the solution. The number twelwe was chosen for reasons related to the number of full moons occuring between each time the earth circles around the sun, so the absolute deadline for finishing everything is on the 31st December 2019.\n ","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"451e93cfac38783ceab213856d9f8809","permalink":"/post/lasso-in-12-languages/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/post/lasso-in-12-languages/","section":"post","summary":"During the holidays, I finally took the time to read The Pragmatic Programmer. The book inspired me to learn some new programming languages, so in 2019, I am going to implement Tibshirani’s lasso in twelve different languages, some of which I know very well, and others in which I have never even said hello world. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the glmnet package.","tags":[],"title":"Lasso in Twelve Languages","type":"post"}]