<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Øystein Sørensen on Øystein Sørensen</title>
    <link>/</link>
    <description>Recent content in Øystein Sørensen on Øystein Sørensen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Lasso in Twelve Languages</title>
      <link>/post/lasso-in-12-languages/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/lasso-in-12-languages/</guid>
      <description>


&lt;p&gt;During the holidays, I finally took the time to read &lt;a href=&#34;https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1546001035&amp;amp;sr=8-1&amp;amp;keywords=pragmatic+programmer&#34;&gt;The Pragmatic Programmer&lt;/a&gt;. The book inspired me to learn some new programming languages, so in 2019, I am going to implement &lt;a href=&#34;http://statweb.stanford.edu/~tibs/lasso.html&#34;&gt;Tibshirani’s lasso&lt;/a&gt; in twelve different languages, some of which I know very well, and others in which I have never even said &lt;em&gt;hello world&lt;/em&gt;. This post is a warm-up, in which I am going to give a brief introduction to the lasso and show how it can be computed very conveniently using the &lt;strong&gt;glmnet&lt;/strong&gt; package.&lt;/p&gt;
&lt;div id=&#34;lasso&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lasso&lt;/h2&gt;
&lt;p&gt;The lasso is a fairly simple and very well-known machine learning algorithm. It imposes an &lt;span class=&#34;math inline&#34;&gt;\(\ell_{1}\)&lt;/span&gt;-penalty on the coefficients of a regression model. In its simplest case, the criterion is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\beta} = \text{arg}~\text{min}_{\beta} \left\| y - X \beta\right\|_{2}^{2} + \lambda \left\|\beta\right\|_{1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a regularization parameter, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a response vector, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a matrix of regression coefficients. Because of the absolute value in the penalty term (&lt;span class=&#34;math inline&#34;&gt;\(\|\beta\|_{1} = \sum_{j} |\beta_{j}|\)&lt;/span&gt;), the lasso estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; typically sets some of the regression coefficients exactly to zero. This yields a sparse solution, in contrast to what an &lt;span class=&#34;math inline&#34;&gt;\(\ell_{2}\)&lt;/span&gt;-penalty would give. This property makes the lasso really useful in a lot of settings, including in &lt;a href=&#34;https://www.deeplearningbook.org/contents/regularization.html&#34;&gt;regularization of neural networks&lt;/a&gt; estimation of graphical model structures, and generalized linear models. The theory of the lasso has been described in lots of papers, much of which is summarized in &lt;a href=&#34;https://www.springer.com/la/book/9783642201912&#34;&gt;this book&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-using-glmnet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution Using glmnet&lt;/h2&gt;
&lt;p&gt;In R, the &lt;strong&gt;glmnet&lt;/strong&gt; package is the main tool for computing the lasso. It is based based on a Fortran implementation of the coordinate descent. When implementing the lasso in various languages, I will use the &lt;a href=&#34;http://lib.stat.cmu.edu/datasets/boston&#34;&gt;Boston house-price data&lt;/a&gt;, and try to predict the median house value using the other variables in the dataset. This dataset is actually also available in R through the &lt;strong&gt;MASS&lt;/strong&gt; package, so in order to have a reference solution, we can execute the following lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glmnet)
library(glmnetUtils)
library(MASS)
fit &amp;lt;- cv.glmnet(medv ~ ., data = Boston)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This estimates the cross-validated loss function of the lasso along a grid of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values. We can visualize it as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-28-lasso-in-twelve-languages_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The numbers in the upper horizontal bar show how many coefficients are nonzero. The rightmost line is the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; corresponding to the “one standard error rule”, and we choose this one as our final value of the regularization parameter. We get the regression coefficients as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(fit, s = &amp;quot;lambda.1se&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 14 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                        1
## (Intercept) 18.567394066
## crim        -0.024001671
## zn           .          
## indus        .          
## chas         1.994355941
## nox         -4.485651769
## rm           4.272373008
## age          .          
## dis         -0.390100181
## rad          .          
## tax          .          
## ptratio     -0.801611503
## black        0.006695178
## lstat       -0.518555953&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, we can find the cross-validation estimate of the mean squared error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$cvm[fit$lambda == fit$lambda.1se]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 26.16966&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the values I will try to reproduce by actually implementing the lasso in twelve different languages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-involved-in-computing-the-lasso&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Steps Involved in Computing the Lasso&lt;/h2&gt;
&lt;p&gt;Packages/libraries/etc. for computing the lasso exist in several languages, but in order to make it a bit more interesting, I am going to implement what &lt;strong&gt;glmnet&lt;/strong&gt; computed for me above by actually coding it. This includes:&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;Download the &lt;a href=&#34;http://lib.stat.cmu.edu/datasets/boston&#34;&gt;Boston Housing Dataset&lt;/a&gt; and convert it to a proper object.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coordinate-descent&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coordinate Descent&lt;/h3&gt;
&lt;p&gt;Implement the &lt;a href=&#34;https://projecteuclid.org/euclid.aoas/1206367819&#34;&gt;coordinate descent algorithm&lt;/a&gt; for estimating &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; at a given &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This involves iterating&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde{\beta}_{j}(\lambda) \leftarrow S\left(\sum_{i=1}^{N} x_{ij}(y_{i} - \tilde{y}_{i}^{(j)}), \lambda \right)\]&lt;/span&gt;
until convergence, where &lt;span class=&#34;math inline&#34;&gt;\(S(\cdot, \cdot)\)&lt;/span&gt; is the soft-thresholding operator and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}_{i}^{(j)}\)&lt;/span&gt; is the current fitted value for observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; with covariate &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; removed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross-Validation&lt;/h3&gt;
&lt;p&gt;Ten-fold cross-validation for estimating the mean-squared prediction error as a function of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Ideally this can be done in parallel.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting&lt;/h3&gt;
&lt;p&gt;Function for plotting the cross-validation curve.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standardization&lt;/h3&gt;
&lt;p&gt;Standardization of covariates before using the algorithm and rescaling to get &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; in the proper scale. This is because all columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; must have the same standard deviation as long as we use a common &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plan&lt;/h2&gt;
&lt;p&gt;My current plan is to implement the lasso in the following languages, partly based on looking at &lt;a href=&#34;https://insights.stackoverflow.com/survey/2018/#most-popular-technologies&#34;&gt;the Stack Overflow Developer Survey&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bash_(Unix_shell)&#34;&gt;Bash&lt;/a&gt; (must be possible…)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/C_(programming_language)&#34;&gt;C&lt;/a&gt; (should be really fast)&lt;/li&gt;
&lt;li&gt;C++ (using classes and stuff, not C-style)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Fortran&#34;&gt;Fortran&lt;/a&gt; (really old and really fast?)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Java_(programming_language)&#34;&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt; (might be the next big thing?)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://common-lisp.net/&#34;&gt;Common Lisp&lt;/a&gt; (because references to Lisp keep showing up in R)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.gnu.org/software/octave/&#34;&gt;GNU Octave&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; (obviously)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.scala-lang.org/&#34;&gt;Scala&lt;/a&gt; (the main language used in Apache Spark, so it must me good to know, right?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will right one blogpost for each, detailing the solution. The number twelwe was chosen for reasons related to the number of full moons occuring between each time the earth circles around the sun, so the absolute deadline for finishing everything is on the 31st December 2019.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
